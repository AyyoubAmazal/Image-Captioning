<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About - AI Image Caption Generator</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        :root {
            --primary-color: #6a5acd;
            --secondary-color: #4b0082;
            --accent-color: #9370db;
            --light-color: #f5f5f5;
            --dark-color: #333;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        body {
            background-image: url('static/images/background.png');
            background-size: cover;
            background-position: center;
            background-attachment: fixed;
            color: var(--light-color);
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        
        body::before {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 30, 0.7);
            z-index: -1;
        }
        
        header {
            background: rgba(0, 0, 0, 0.7);
            padding: 20px 0;
            position: relative;
        }
        
        nav {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0 20px;
        }
        
        .logo {
            display: flex;
            align-items: center;
            color: var(--accent-color);
            font-size: 24px;
            font-weight: bold;
        }
        
        .logo i {
            margin-right: 10px;
            font-size: 28px;
        }
        
        .nav-links {
            display: flex;
            list-style: none;
        }
        
        .nav-links li a {
            color: var(--light-color);
            text-decoration: none;
            padding: 10px 20px;
            margin: 0 5px;
            border-radius: 30px;
            transition: all 0.3s ease;
        }
        
        .nav-links li a:hover,
        .nav-links li a.active {
            background: var(--primary-color);
            color: white;
        }
        
        .container {
            max-width: 1200px;
            margin: 40px auto;
            padding: 0 20px;
            flex: 1;
        }
        
        .about-section {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
            max-width: 900px;
            margin: 0 auto;
            animation: slideUp 0.8s ease-out;
        }
        
        .section-title {
            text-align: center;
            margin-bottom: 40px;
            position: relative;
            font-size: 2.5rem;
            color: white;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
        }
        
        .section-title::after {
            content: '';
            position: absolute;
            width: 100px;
            height: 4px;
            background: var(--accent-color);
            bottom: -10px;
            left: 50%;
            transform: translateX(-50%);
            border-radius: 2px;
        }
        
        .about-content {
            display: grid;
            grid-template-columns: 1fr 1fr;
            grid-gap: 30px;
            align-items: center;
        }
        
        .about-image {
            position: relative;
            overflow: hidden;
            border-radius: 10px;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
        }
        
        .about-image img {
            width: 100%;
            height: auto;
            transition: transform 0.5s ease;
            display: block;
        }
        
        .about-image:hover img {
            transform: scale(1.05);
        }
        
        .about-text p {
            margin-bottom: 20px;
            line-height: 1.7;
            color: rgba(255, 255, 255, 0.9);
        }
        
        .info-section {
            margin-top: 60px;
        }
        
        .info-cards {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            grid-gap: 30px;
            margin-top: 40px;
        }
        
        .info-card {
            background: rgba(0, 0, 0, 0.4);
            padding: 30px;
            border-radius: 15px;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            border-left: 4px solid var(--accent-color);
        }
        
        .info-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.4);
        }
        
        .info-card i {
            font-size: 2.5rem;
            color: var(--accent-color);
            margin-bottom: 20px;
        }
        
        .info-card h3 {
            font-size: 1.4rem;
            margin-bottom: 15px;
            color: white;
        }
        
        .info-card p {
            line-height: 1.6;
            color: rgba(255, 255, 255, 0.8);
        }
        
        .model-architecture {
            margin-top: 60px;
            background: rgba(0, 0, 0, 0.4);
            padding: 30px;
            border-radius: 15px;
        }
        
        .model-architecture h3 {
            font-size: 1.6rem;
            margin-bottom: 20px;
            text-align: center;
            color: white;
        }
        
        .model-diagram {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin-top: 30px;
        }
        
        .diagram-step {
            width: 80%;
            padding: 20px;
            margin-bottom: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
            position: relative;
            display: flex;
            align-items: center;
        }
        
        .diagram-step:not(:last-child)::after {
            content: '';
            position: absolute;
            width: 4px;
            height: 30px;
            background: var(--accent-color);
            bottom: -30px;
            left: 50%;
            transform: translateX(-50%);
        }
        
        .step-number {
            width: 40px;
            height: 40px;
            background: var(--accent-color);
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            font-weight: bold;
            margin-right: 20px;
            flex-shrink: 0;
        }
        
        .step-content {
            flex: 1;
        }
        
        .step-content h4 {
            margin-bottom: 10px;
            color: white;
        }
        
        .technical-details {
            margin-top: 60px;
        }
        
        footer {
            background: rgba(0, 0, 0, 0.8);
            padding: 20px 0;
            text-align: center;
            margin-top: auto;
        }
        
        .footer-content {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        
        .footer-content p {
            color: rgba(255, 255, 255, 0.7);
        }
        
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
        
        @keyframes slideUp {
            from { 
                opacity: 0;
                transform: translateY(50px);
            }
            to { 
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        @media (max-width: 768px) {
            .about-content {
                grid-template-columns: 1fr;
            }
            
            .diagram-step {
                width: 100%;
            }
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <div class="logo">
                <i class="fas fa-brain"></i>
                <span>AI Caption Pro</span>
            </div>
            <ul class="nav-links">
                <li><a href="/">Home</a></li>
                <li><a href="/about" class="active">About</a></li>
            </ul>
        </nav>
    </header>

    <div class="container">
        <div class="about-section">
            <h2 class="section-title">About Our Technology</h2>
            
            <div class="about-content">
                <div class="about-image">
                    <img src="static/images/ai-vision.png" alt="AI Vision Technology">
                </div>
                <div class="about-text">
                    <p>Our AI Image Caption Generator uses state-of-the-art deep learning technology to automatically generate human-like descriptions for any image. This advanced system combines computer vision and natural language processing to understand visual content and express it in natural language.</p>
                    <p>We offer three different model architectures to generate captions: CNN-BiLSTM (using InceptionV3), VGG19-LSTM, and BLIP. Each model has its own strengths and may produce different results for the same image, giving you options to choose the best caption for your needs.</p>
                </div>
            </div>
            
            <div class="info-section">
                <div class="info-cards">
                    <div class="info-card">
                        <i class="fas fa-eye"></i>
                        <h3>Computer Vision</h3>
                        <p>Our systems use pre-trained CNNs (InceptionV3 or VGG19) to analyze visual elements including objects, actions, scenes, colors, and spatial relationships within any image you upload.</p>
                    </div>
                    
                    <div class="info-card">
                        <i class="fas fa-language"></i>
                        <h3>Natural Language Processing</h3>
                        <p>The LSTM and BiLSTM components process visual features and convert them into natural, human-like language that accurately describes the content of your images.</p>
                    </div>
                    
                    <div class="info-card">
                        <i class="fas fa-cogs"></i>
                        <h3>Deep Learning</h3>
                        <p>Our models are trained on millions of image-caption pairs, allowing them to learn complex patterns and relationships between visual content and textual descriptions.</p>
                    </div>
                </div>
            </div>
            
            <div class="model-architecture">
                <h3>CNN-BiLSTM Model Architecture</h3>
                
                <div class="model-diagram">
                    <div class="diagram-step">
                        <div class="step-number">1</div>
                        <div class="step-content">
                            <h4>Image Feature Extraction</h4>
                            <p>The input image is processed through InceptionV3, a powerful CNN pre-trained on the ImageNet dataset, to extract high-level visual features.</p>
                        </div>
                    </div>
                    
                    <div class="diagram-step">
                        <div class="step-number">2</div>
                        <div class="step-content">
                            <h4>Feature Encoding</h4>
                            <p>The extracted features are encoded into a format suitable for language processing, capturing the essential visual content in a compact representation.</p>
                        </div>
                    </div>
                    
                    <div class="diagram-step">
                        <div class="step-number">3</div>
                        <div class="step-content">
                            <h4>Bidirectional LSTM Processing</h4>
                            <p>A bidirectional LSTM network processes the encoded features to generate contextually appropriate words, considering both past and future words in the sequence.</p>
                        </div>
                    </div>
                    
                    <div class="diagram-step">
                        <div class="step-number">4</div>
                        <div class="step-content">
                            <h4>Caption Generation</h4>
                            <p>The final caption is generated word by word, with each word being selected based on the image features and previously generated words, ensuring coherent and relevant descriptions.</p>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="model-architecture" style="margin-top: 60px;">
                <h3>VGG19-LSTM Model Architecture</h3>
                
                <div class="model-diagram">
                    <div class="diagram-step">
                        <div class="step-number">1</div>
                        <div class="step-content">
                            <h4>Feature Extraction with VGG19</h4>
                            <p>The image is processed through VGG19, a deep CNN with 19 layers, pre-trained on ImageNet. Features are extracted from the fc2 layer (4096-dimensional vector).</p>
                        </div>
                    </div>
                    
                    <div class="diagram-step">
                        <div class="step-number">2</div>
                        <div class="step-content">
                            <h4>Feature Transformation</h4>
                            <p>The 4096-dimensional features are processed and transformed to be compatible with the language generation model.</p>
                        </div>
                    </div>
                    
                    <div class="diagram-step">
                        <div class="step-number">3</div>
                        <div class="step-content">
                            <h4>LSTM Sequence Processing</h4>
                            <p>An LSTM network processes the visual features along with previously generated words to predict the next word in the caption sequence.</p>
                        </div>
                    </div>
                    
                    <div class="diagram-step">
                        <div class="step-number">4</div>
                        <div class="step-content">
                            <h4>Sequential Caption Generation</h4>
                            <p>Starting with a special 'start' token, the model generates the caption one word at a time until it produces an 'end' token or reaches the maximum length.</p>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="technical-details">
                <h3 class="section-title">Technical Specifications</h3>
                
                <div class="info-cards">
                    <div class="info-card">
                        <i class="fas fa-microchip"></i>
                        <h3>CNN-BiLSTM Model</h3>
                        <p>Our CNN-BiLSTM model uses InceptionV3 for feature extraction, followed by a bidirectional LSTM with 256 hidden units. The vocabulary includes over 10,000 words for rich and precise descriptions.</p>
                    </div>
                    
                    <div class="info-card">
                        <i class="fas fa-microchip"></i>
                        <h3>VGG19-LSTM Model</h3>
                        <p>The VGG19-LSTM model uses VGG19's fc2 layer for 4096-dimensional feature extraction, followed by a deep LSTM network. This architecture is known for capturing fine-grained details in images.</p>
                    </div>
                    
                    <div class="info-card">
                        <i class="fas fa-graduation-cap"></i>
                        <h3>Training Data</h3>
                        <p>Both CNN-BiLSTM and VGG19-LSTM models are trained on Flickr30k datasets, comprising millions of image-caption pairs for robust performance.</p>
                    </div>
                </div>
            </div>
            
            <div class="technical-details" style="margin-top: 60px;">
                <h3 class="section-title">BLIP Model</h3>
                <p style="text-align: center; margin-bottom: 30px;">We also provide the powerful BLIP (Bootstrapping Language-Image Pre-training) model as an alternative option for image captioning.</p>
                
                <div class="info-cards">
                    <div class="info-card">
                        <i class="fas fa-brain"></i>
                        <h3>How BLIP Works</h3>
                        <p>BLIP is a vision-language model pre-trained on a large dataset of image-text pairs. It uses a transformer-based architecture to effectively bridge visual and textual understanding.</p>
                    </div>
                    
                    <div class="info-card">
                        <i class="fas fa-tachometer-alt"></i>
                        <h3>BLIP Advantages</h3>
                        <p>The BLIP model often produces more natural-sounding, detailed captions compared to traditional approaches. It excels at capturing nuances in images that other models might miss.</p>
                    </div>
                    
                    <div class="info-card">
                        <i class="fas fa-code-branch"></i>
                        <h3>Model Architecture</h3>
                        <p>BLIP uses a ViT (Vision Transformer) backbone for image encoding and a text decoder based on BERT. This unified architecture allows for effective multi-modal understanding and generation.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <footer>
        <div class="footer-content">
            <p>&copy; 2025 AI Caption Pro.</p>
        </div>
    </footer>
</body>
</html>